{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=['a glass of beer',\n",
    "     'the glass of juice',\n",
    "     'welcome to my place',\n",
    "     'I am learning python',\n",
    "     'understand the meaning of words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a glass of beer',\n",
       " 'the glass of juice',\n",
       " 'welcome to my place',\n",
       " 'I am learning python',\n",
       " 'understand the meaning of words']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a vocabulary size- dictionary of size 10000\n",
    "voc_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8754, 1665, 2895, 4662],\n",
       " [8611, 1665, 2895, 411],\n",
       " [7478, 6788, 3614, 1942],\n",
       " [4702, 3420, 8294, 8630],\n",
       " [1485, 8611, 1972, 2895, 6449]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying one hot function where only 1 index(where the word is present) is 1 and rest is 0\n",
    "# onehot() function will convert each sentence to 's and 1's considering some vocabulary size and it will give index \n",
    "# position of each word in the sentence\n",
    "onehot_words = []\n",
    "for sentence in sent:\n",
    "    onehot_words.append(one_hot(sentence, voc_size))\n",
    "    # Arguments should be each sentence and the vocabulary size\n",
    "    \n",
    "onehot_words\n",
    "# These numbers are the index(index from the dictionary) of each word in a sentence based on given vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever passing any sentences to the embedding layer all the sentences must have the same number of words. This will help to create a good embedding matrix.\n",
    "- To make this happen we are using pad_sequences, ie to make words equal in all the sentences\n",
    "- 0(zero) is added after or before all the words, ie the word at 0th index will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 8754, 1665, 2895, 4662],\n",
       "       [   0,    0,    0,    0, 8611, 1665, 2895,  411],\n",
       "       [   0,    0,    0,    0, 7478, 6788, 3614, 1942],\n",
       "       [   0,    0,    0,    0, 4702, 3420, 8294, 8630],\n",
       "       [   0,    0,    0, 1485, 8611, 1972, 2895, 6449]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_length = 8\n",
    "# We want to have maximum of 8 words in each sentence\n",
    "embedded_docs = pad_sequences(onehot_words, padding = 'pre', maxlen = sent_length)\n",
    "# first argument: passing one hot encoded values\n",
    "# second argument: padding = 'pre' - padding is added before all the words\n",
    "    # padding = 'post' - padding is added after all the words\n",
    "# third argument: maxlen = max length of the sentence\n",
    "embedded_docs\n",
    "# Now we will pass this matrix to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of dimensions to be used\n",
    "dim = 10\n",
    "# This is basically the number of features which we take for FEATURE REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "model = Sequential()\n",
    "# Adding the embedding layer\n",
    "model.add(Embedding(voc_size, dim, input_length = sent_length))\n",
    "# First argument = dictionary size\n",
    "# Second argument = dimensions which are to be taken for feature representation\n",
    "# Each word in every sentence will be converted to the given dimension\n",
    "# So for first sentence, word present at 705 index(the) will be converted to vectors of 10 dimension\n",
    "# Third argument = Maximum length of sentence\n",
    "\n",
    "# This embedding layer will create a Feature Representation for each word in every sentence\n",
    "\n",
    "model.compile(optimizer = 'adam', metrics = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 8, 10)             100000    \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer.\n",
    "\n",
    "- The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
    "\n",
    "- If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [ 0.03038326 -0.04392822  0.02806565 -0.02126027 -0.00138902\n",
      "    0.01621561 -0.04730344 -0.03245162 -0.02034541  0.01248621]\n",
      "  [ 0.01415369  0.02820823 -0.00216037  0.0408794   0.043222\n",
      "   -0.04473099  0.04390018  0.03240642 -0.01373659  0.0081668 ]\n",
      "  [ 0.02760471  0.01612211  0.00382591  0.0421035   0.01919608\n",
      "   -0.02776971 -0.02248905 -0.03643564  0.00377132  0.01732625]\n",
      "  [-0.04415665 -0.04150409 -0.03169983 -0.02090961  0.03045875\n",
      "   -0.01201976 -0.04392027  0.03880182  0.04730037  0.00900483]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [ 0.01615227  0.03037752 -0.00405779  0.00416575 -0.00852118\n",
      "    0.04599509 -0.04676824  0.02574794 -0.03694499  0.03132923]\n",
      "  [ 0.01415369  0.02820823 -0.00216037  0.0408794   0.043222\n",
      "   -0.04473099  0.04390018  0.03240642 -0.01373659  0.0081668 ]\n",
      "  [ 0.02760471  0.01612211  0.00382591  0.0421035   0.01919608\n",
      "   -0.02776971 -0.02248905 -0.03643564  0.00377132  0.01732625]\n",
      "  [ 0.02672136  0.01994277  0.04415964 -0.01303816  0.03595627\n",
      "   -0.02186563 -0.00219729 -0.02520403  0.0464846  -0.03986762]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.04490066 -0.01777371 -0.02171419 -0.01413916 -0.00542367\n",
      "    0.04814109 -0.00202241 -0.00851895  0.04966186  0.03960148]\n",
      "  [-0.04693481 -0.00640626  0.00238531  0.04099771  0.02646628\n",
      "    0.0254412  -0.00028063  0.02381594  0.02977187 -0.00979667]\n",
      "  [-0.01658505 -0.00854064 -0.01363379 -0.00796626  0.04677394\n",
      "    0.01611121 -0.00895909  0.0422481  -0.01144207 -0.04802164]\n",
      "  [ 0.0424824   0.03324438 -0.02100976  0.00791566  0.00797807\n",
      "   -0.0307495   0.00430943 -0.0093016  -0.03623474 -0.03577698]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.03246181  0.03737099 -0.02813032 -0.0341676  -0.01612885\n",
      "    0.0058396  -0.00995827  0.03294053 -0.00221067  0.00967169]\n",
      "  [-0.02337405  0.03556843  0.00864626 -0.00554151  0.02035258\n",
      "   -0.02279565  0.03351304 -0.0038284   0.01748891  0.04243996]\n",
      "  [ 0.01447843  0.02321506  0.00024676  0.04799781  0.03798981\n",
      "   -0.02934587  0.04822023 -0.03944016  0.04774555  0.01722677]\n",
      "  [ 0.01728285 -0.02097608  0.04967511  0.02843529  0.02732049\n",
      "   -0.04381274  0.00861507 -0.02417259  0.01604177  0.02187606]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]\n",
      "  [ 0.03285774 -0.01223383 -0.03441383 -0.00768747  0.00510498\n",
      "   -0.04456716  0.02748311 -0.03847776  0.03137868 -0.03866736]\n",
      "  [ 0.01615227  0.03037752 -0.00405779  0.00416575 -0.00852118\n",
      "    0.04599509 -0.04676824  0.02574794 -0.03694499  0.03132923]\n",
      "  [-0.01433898  0.00192078 -0.0234519   0.03666185 -0.0273154\n",
      "   -0.00168232 -0.02584708 -0.03200756 -0.03940783  0.01886157]\n",
      "  [ 0.02760471  0.01612211  0.00382591  0.0421035   0.01919608\n",
      "   -0.02776971 -0.02248905 -0.03643564  0.00377132  0.01732625]\n",
      "  [-0.01097738  0.02234605  0.00938662 -0.02439407 -0.03293236\n",
      "    0.00212266 -0.01505502 -0.0114659  -0.0043697  -0.03611831]]]\n"
     ]
    }
   ],
   "source": [
    "# To see the embedded vector values\n",
    "print(model.predict(embedded_docs))\n",
    "# For each word 10 dimension vector is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 8754, 1665, 2895, 4662])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]\n",
    "# first sentence with added padding\n",
    "#  0 0 0 0 the(705) glass(2708) of(9328) milk(8434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 8) for input Tensor(\"embedding_6_input:0\", shape=(None, 8), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "[[[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]]\n",
      "\n",
      " [[-0.00185369 -0.02906373 -0.03831832  0.00635027 -0.02974092\n",
      "   -0.00651143  0.0255412   0.0060045  -0.00444395 -0.02586404]]\n",
      "\n",
      " [[ 0.03038326 -0.04392822  0.02806565 -0.02126027 -0.00138902\n",
      "    0.01621561 -0.04730344 -0.03245162 -0.02034541  0.01248621]]\n",
      "\n",
      " [[ 0.01415369  0.02820823 -0.00216037  0.0408794   0.043222\n",
      "   -0.04473099  0.04390018  0.03240642 -0.01373659  0.0081668 ]]\n",
      "\n",
      " [[ 0.02760471  0.01612211  0.00382591  0.0421035   0.01919608\n",
      "   -0.02776971 -0.02248905 -0.03643564  0.00377132  0.01732625]]\n",
      "\n",
      " [[-0.04415665 -0.04150409 -0.03169983 -0.02090961  0.03045875\n",
      "   -0.01201976 -0.04392027  0.03880182  0.04730037  0.00900483]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs[0]))\n",
    "# Here we can see that each word in a sentence is converted to 10 vectors as we have given the dimension as 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 19], [19, 12], [40, 30], [8, 12], [37], [14], [25, 30], [19, 19], [25, 12], [19, 49, 19, 27]]\n",
      "[[18 19  0  0]\n",
      " [19 12  0  0]\n",
      " [40 30  0  0]\n",
      " [ 8 12  0  0]\n",
      " [37  0  0  0]\n",
      " [14  0  0  0]\n",
      " [25 30  0  0]\n",
      " [19 19  0  0]\n",
      " [25 12  0  0]\n",
      " [19 49 19 27]]\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 945\n",
      "Trainable params: 945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x00000126B5C19598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "# Giving shape also for first hidden layer\n",
    "model.add(Dense(16, activation = 'relu', input_shape = (32,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
